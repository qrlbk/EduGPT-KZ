---
watermark: "EduGPT KZ Educational AI - Adilet Kuralbek ¬© 2025"
copyright: "Copyright ¬© 2025 Adilet Kuralbek. All rights reserved."
license: "Proprietary software - Commercial use only"
contact: "Contact: adilet.kuralbek@email.com"
---

# ‚ö° PERFORMANCE OPTIMIZATION PLAN

**–¶–µ–ª—å:** p95 < 5s (–≤–º–µ—Å—Ç–æ 25s)  
**–°—Ä–æ–∫:** 2-3 –¥–Ω—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** CRITICAL

---

## üéØ –¢–ï–ö–£–©–ò–ï –ü–†–û–ë–õ–ï–ú–´

### Problem #1: Retrieval 12s (–ö–ê–¢–ê–°–¢–†–û–§–ê!)

**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å–µ–π—á–∞—Å:**
```python
# backend/services/hybrid_search.py
async def search(query, faq_items, language, top_k=10):
    # 1. Query Expansion: 3s (GPT call)
    expanded = await query_expander.expand(query)
    
    # 2. Embeddings: 5s (5√ó OpenAI API calls!)
    for variant in expanded.variants:
        embedding = await openai.embeddings.create(...)  # 1s –∫–∞–∂–¥—ã–π!
    
    # 3. BM25: 100ms (OK)
    bm25_results = bm25_search.search(query)
    
    # 4. Semantic: 3s (—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å 62 FAQ)
    semantic_results = cosine_similarity(query_emb, faq_embeddings)
    
    # 5. RRF merge: 4s (200+ documents!)
    merged = rrf_merge(bm25_results, semantic_results)
    
    return merged[:top_k]
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
- ‚ùå Runtime embeddings (5√ó API calls)
- ‚ùå Query Expansion –∫–∞–∂–¥—ã–π —Ä–∞–∑ (GPT)
- ‚ùå RRF merge 200+ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- ‚ùå –ù–µ—Ç –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è

---

### Problem #2: GPT 9s

**–ü—Ä–æ–±–ª–µ–º—ã:**
- ‚ùå –û—Ç–ø—Ä–∞–≤–ª—è–µ–º top-10 FAQ (5750 bytes)
- ‚ùå –î–ª–∏–Ω–Ω—ã–π system prompt
- ‚ùå No streaming
- ‚ùå max_tokens=1000 (–º–æ–∂–Ω–æ –º–µ–Ω—å—à–µ)

---

### Problem #3: DB Save 5s

**–ü—Ä–æ–±–ª–µ–º—ã:**
- ‚ùå 3 –æ—Ç–¥–µ–ª—å–Ω—ã—Ö SELECT
- ‚ùå –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å (–±–ª–æ–∫–∏—Ä—É–µ—Ç handler)
- ‚ùå –ù–µ—Ç –∏–Ω–¥–µ–∫—Å–æ–≤?

---

## ‚úÖ SOLUTION #1: FAST RETRIEVAL (<600ms)

### Step 1.1: Pre-computed Embeddings

**–°–µ–π—á–∞—Å:**
```python
# Runtime embedding (1s)
query_embedding = await openai.embeddings.create(input=query)
```

**–°—Ç–∞–ª–æ:**
```python
# –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ embeddings –≤ PostgreSQL —Å pgvector
CREATE EXTENSION vector;

CREATE TABLE faq_embeddings (
    faq_id UUID PRIMARY KEY,
    question_embedding vector(1536),  -- OpenAI dimension
    created_at TIMESTAMP
);

-- HNSW –∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
CREATE INDEX ON faq_embeddings 
USING hnsw (question_embedding vector_cosine_ops);

# –í –∫–æ–¥–µ:
query_emb = await get_cached_embedding(query)  # 50ms
results = await db.execute(
    """
    SELECT faq_id, 1 - (question_embedding <=> :query_emb) as similarity
    FROM faq_embeddings
    ORDER BY question_embedding <=> :query_emb
    LIMIT 50
    """,
    {"query_emb": query_emb}
)  # 100ms —Å HNSW!
```

**Gain:** 5s ‚Üí 150ms (-97%)

---

### Step 1.2: Two-Stage Retrieval

```python
async def fast_retrieval(query: str, language: Language, top_k: int = 10):
    """–î–≤—É—Ö—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–π —Ä–µ—Ç—Ä–∏–≤: BM25 ‚Üí Vector"""
    
    # Stage 1: BM25 (Postgres FTS) - –±—ã—Å—Ç—Ä–æ, –≥—Ä—É–±–æ
    bm25_results = await db.execute(
        """
        SELECT faq_id, ts_rank(fts_vector, plainto_tsquery(:query)) as rank
        FROM faq_items
        WHERE fts_vector @@ plainto_tsquery(:query)
        ORDER BY rank DESC
        LIMIT 50
        """,
        {"query": query}
    )  # 50ms
    
    # Stage 2: Vector rerank —Ç–æ–ª—å–∫–æ top-50
    if len(bm25_results) < 10:
        # Fallback: —á–∏—Å—Ç—ã–π semantic search
        query_emb = await get_cached_embedding(query)
        vector_results = await vector_search(query_emb, limit=50)
        candidates = vector_results
    else:
        candidates = bm25_results
    
    # Rerank top-50 (–Ω–µ 200!)
    faq_ids = [r['faq_id'] for r in candidates]
    query_emb = await get_cached_embedding(query)
    
    final_results = await db.execute(
        """
        SELECT f.*, 
               1 - (e.question_embedding <=> :query_emb) as similarity
        FROM faq_items f
        JOIN faq_embeddings e ON f.id = e.faq_id
        WHERE f.id = ANY(:faq_ids)
        ORDER BY similarity DESC
        LIMIT :top_k
        """,
        {"query_emb": query_emb, "faq_ids": faq_ids, "top_k": top_k}
    )  # 100ms
    
    return final_results

# –ò–¢–û–ì–û: 50ms + 100ms + 100ms = 250ms!
```

**Gain:** 12s ‚Üí 250ms (-95%)

---

### Step 1.3: –£–±—Ä–∞—Ç—å Query Expansion

```python
# –°–ï–ô–ß–ê–°:
expanded = await query_expander.expand(query)  # 3s!

# –°–¢–ê–õ–û:
# –£–±–∏—Ä–∞–µ–º! Semantic search —Å–∞–º –Ω–∞–π–¥–µ—Ç —Å–∏–Ω–æ–Ω–∏–º—ã
# –ï—Å–ª–∏ –æ—á–µ–Ω—å –Ω—É–∂–Ω–æ - –¥–µ–ª–∞—Ç—å OFFLINE –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ FAQ
```

**Gain:** 3s ‚Üí 0s

---

### Step 1.4: Redis Cache –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤

```python
import hashlib

async def get_cached_faq_results(query: str, language: Language):
    """–ö–µ—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
    normalized = query.lower().strip()
    cache_key = f"faq_results:{language.value}:{hashlib.blake2b(normalized.encode()).hexdigest()[:16]}"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
    cached = await redis.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # –ï—Å–ª–∏ –Ω–µ—Ç - –¥–µ–ª–∞–µ–º –ø–æ–∏—Å–∫
    results = await fast_retrieval(query, language)
    
    # –ö–µ—à–∏—Ä—É–µ–º –Ω–∞ 30 –º–∏–Ω—É—Ç
    await redis.setex(cache_key, 1800, json.dumps(results))
    
    return results
```

**Gain –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤:** 250ms ‚Üí 10ms (-96%)

---

### –ò–¢–û–ì–û RETRIEVAL:

```
–ë–´–õ–û:    12,000ms
–°–¢–ê–õ–û:      250ms (–±–µ–∑ –∫–µ—à–∞)
           10ms (—Å –∫–µ—à–µ–º)

–£–°–ö–û–†–ï–ù–ò–ï: 48√ó - 1200√ó!
```

---

## ‚úÖ SOLUTION #2: FAST GPT (9s ‚Üí 2-3s)

### Step 2.1: –°–æ–∫—Ä–∞—Ç–∏—Ç—å –ø—Ä–æ–º–ø—Ç

```python
# –°–ï–ô–ß–ê–°: –û—Ç–ø—Ä–∞–≤–ª—è–µ–º 10 FAQ (5750 bytes)
faq_json = json.dumps(clean_faq_items, ensure_ascii=False, indent=2)

# –°–¢–ê–õ–û: –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ 3-5 —Ç–æ–ø–æ–≤—ã—Ö FAQ
top_faq = clean_faq_items[:5]  # –¢–æ–ª—å–∫–æ top-5
faq_json = json.dumps(top_faq, ensure_ascii=False)  # –ë–µ–∑ indent!
```

**Gain:** -30% —Ç–æ–∫–µ–Ω–æ–≤ ‚Üí -30% –≤—Ä–µ–º–µ–Ω–∏

---

### Step 2.2: –°–æ–∫—Ä–∞—Ç–∏—Ç—å system prompt

```python
# –°–ï–ô–ß–ê–°: 200+ —Å—Ç—Ä–æ–∫ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞

# –°–¢–ê–õ–û: –ö—Ä–∞—Ç–∫–∏–π –∏ –ø–æ –¥–µ–ª—É
system_prompt = """–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ –ë—É–∫–µ—Ç–æ–≤–∞.

–ü–†–ê–í–ò–õ–ê:
1. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ FAQ –Ω–∏–∂–µ
2. –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å - —Å–∫–∞–∂–∏ "–ù–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"
3. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ —Ç–æ—á–Ω–æ

FAQ:
{faq_json}

–í–æ–ø—Ä–æ—Å: {query}
"""
```

**Gain:** -50% —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–æ–º–ø—Ç–µ

---

### Step 2.3: –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å max_tokens

```python
# –°–ï–ô–ß–ê–°:
max_tokens=1000

# –°–¢–ê–õ–û:
max_tokens=500  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –∫—Ä–∞—Ç–∫–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
```

**Gain:** -50% –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

---

### Step 2.4: Streaming (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

```python
async def stream_gpt_response(message: Message, query: str, faq: list):
    """Streaming –æ—Ç–≤–µ—Ç–∞ –æ—Ç GPT"""
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º "–ø–µ—á–∞—Ç–∞–µ—Ç..."
    status_msg = await message.answer("‚è≥ –î—É–º–∞—é...")
    
    chunks = []
    async for chunk in await openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[...],
        stream=True
    ):
        if chunk.choices[0].delta.content:
            chunks.append(chunk.choices[0].delta.content)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞–∂–¥—ã–µ 50 —á–∞–Ω–∫–æ–≤
            if len(chunks) % 50 == 0:
                await status_msg.edit_text("".join(chunks))
    
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
    full_text = "".join(chunks)
    await status_msg.edit_text(full_text)
```

**UX Gain:** –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤–∏–¥–∏—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å!

---

### –ò–¢–û–ì–û GPT:

```
–ë–´–õ–û:    9,000ms
–°–¢–ê–õ–û:   2,500ms (–±–µ–∑ streaming)
         1,500ms (first token with streaming)

–£–°–ö–û–†–ï–ù–ò–ï: 3.6√ó - 6√ó
```

---

## ‚úÖ SOLUTION #3: FAST DB (5s ‚Üí 100-150ms)

### Step 3.1: Async Queue –¥–ª—è –∑–∞–ø–∏—Å–∏

```python
# backend/services/message_queue.py
import asyncio
from asyncio import Queue

class MessageQueue:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ—á–µ—Ä–µ–¥—å –¥–ª—è –∑–∞–ø–∏—Å–∏ —Å–æ–æ–±—â–µ–Ω–∏–π"""
    
    def __init__(self, db_session):
        self.queue = Queue()
        self.db_session = db_session
        self.worker_task = None
    
    async def start(self):
        """–ó–∞–ø—É—Å–∫ –≤–æ—Ä–∫–µ—Ä–∞"""
        self.worker_task = asyncio.create_task(self._worker())
    
    async def enqueue(self, telegram_user_id: int, content: str, is_from_user: bool):
        """–î–æ–±–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –æ—á–µ—Ä–µ–¥—å"""
        await self.queue.put({
            'telegram_user_id': telegram_user_id,
            'content': content,
            'is_from_user': is_from_user,
            'timestamp': datetime.utcnow()
        })
    
    async def _worker(self):
        """–í–æ—Ä–∫–µ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—á–µ—Ä–µ–¥–∏"""
        while True:
            try:
                # –ë–µ—Ä–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ –æ—á–µ—Ä–µ–¥–∏
                msg = await self.queue.get()
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
                await self._save_to_db(msg)
                
                self.queue.task_done()
            except Exception as e:
                logger.error(f"Error in message queue worker: {e}")
    
    async def _save_to_db(self, msg: dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
        try:
            # –û–î–ù–ò–ú –∑–∞–ø—Ä–æ—Å–æ–º —Å joinedload
            result = await self.db_session.execute(
                select(User)
                .where(User.telegram_id == msg['telegram_user_id'])
                .options(joinedload(User.chats))
            )
            user = result.scalar_one_or_none()
            
            if not user:
                return
            
            chat = user.chats[0] if user.chats else None
            if not chat:
                return
            
            # –°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
            db_message = DBMessage(
                chat_id=chat.id,
                content=msg['content'],
                is_from_user=msg['is_from_user']
            )
            
            self.db_session.add(db_message)
            await self.db_session.commit()
            
        except Exception as e:
            logger.error(f"Error saving message: {e}")
            await self.db_session.rollback()


# –í handlers/faq.py:

# –°–ï–ô–ß–ê–°:
await save_bot_response_to_db(user_id, response.content)  # 5s –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞!

# –°–¢–ê–õ–û:
await message_queue.enqueue(user_id, response.content, is_from_user=False)  # 1ms!
```

**Gain:** 5s ‚Üí 1ms (-99.98%)

---

### Step 3.2: –ò–Ω–¥–µ–∫—Å—ã

```sql
-- –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏–Ω–¥–µ–∫—Å—ã
SELECT tablename, indexname, indexdef 
FROM pg_indexes 
WHERE schemaname = 'public';

-- –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ
CREATE INDEX CONCURRENTLY IF NOT EXISTS 
    idx_users_telegram_id ON users(telegram_id);

CREATE INDEX CONCURRENTLY IF NOT EXISTS 
    idx_chats_user_id ON chats(user_id);

CREATE INDEX CONCURRENTLY IF NOT EXISTS 
    idx_messages_chat_id ON messages(chat_id);

CREATE INDEX CONCURRENTLY IF NOT EXISTS 
    idx_messages_created_at ON messages(created_at DESC);

-- Full-Text Search –¥–ª—è BM25
CREATE INDEX CONCURRENTLY IF NOT EXISTS 
    idx_faq_fts ON faq_items USING gin(fts_vector);

-- ANALYZE –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
ANALYZE users;
ANALYZE chats;
ANALYZE messages;
ANALYZE faq_items;
```

---

### –ò–¢–û–ì–û DB:

```
–ë–´–õ–û:    5,000ms (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å)
–°–¢–ê–õ–û:      1ms (async queue)

–£–°–ö–û–†–ï–ù–ò–ï: 5000√ó!
```

---

## üìä –§–ò–ù–ê–õ–¨–ù–´–ô BREAKDOWN

```
–ö–û–ú–ü–û–ù–ï–ù–¢          –ë–´–õ–û      –°–¢–ê–õ–û     GAIN
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Guardrails         100ms     100ms     0√ó
Retrieval       12,000ms     250ms    48√ó
‚îú‚îÄ BM25             100ms      50ms     2√ó
‚îú‚îÄ Query Exp      3,000ms       0ms     ‚àû
‚îú‚îÄ Embeddings     5,000ms      50ms   100√ó
‚îî‚îÄ RRF merge      4,000ms     150ms    27√ó
GPT              9,000ms   2,500ms   3.6√ó
DB Save          5,000ms       1ms  5000√ó
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL           25,000ms   2,851ms   8.8√ó

p50:  25s ‚Üí 2.9s  ‚úÖ
p95:  30s ‚Üí 4.5s  ‚úÖ
p99:  40s ‚Üí 6.0s  ‚úÖ
```

**–ò–¢–û–ì–û: 8.8√ó –£–°–ö–û–†–ï–ù–ò–ï!**

---

## üîß IMPLEMENTATION CHECKLIST

### Phase 1: DB (1 –¥–µ–Ω—å)

- [ ] –°–æ–∑–¥–∞—Ç—å `faq_embeddings` —Ç–∞–±–ª–∏—Ü—É —Å pgvector
- [ ] –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–∏—Ç—å –≤—Å–µ embeddings (batch)
- [ ] –°–æ–∑–¥–∞—Ç—å HNSW –∏–Ω–¥–µ–∫—Å
- [ ] –î–æ–±–∞–≤–∏—Ç—å FTS –∏–Ω–¥–µ–∫—Å –¥–ª—è BM25
- [ ] –°–æ–∑–¥–∞—Ç—å MessageQueue –∫–ª–∞—Å—Å
- [ ] –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å queue –≤ handlers

### Phase 2: Retrieval (1 –¥–µ–Ω—å)

- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `fast_retrieval()` —Å two-stage
- [ ] –î–æ–±–∞–≤–∏—Ç—å Redis –∫–µ—à –¥–ª—è embeddings
- [ ] –£–±—Ä–∞—Ç—å Query Expansion
- [ ] –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å RRF –¥–æ top-50
- [ ] –î–æ–±–∞–≤–∏—Ç—å –∫–µ—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞

### Phase 3: GPT (0.5 –¥–Ω—è)

- [ ] –°–æ–∫—Ä–∞—Ç–∏—Ç—å –¥–æ top-5 FAQ
- [ ] –£–ø—Ä–æ—Å—Ç–∏—Ç—å system prompt
- [ ] –£–º–µ–Ω—å—à–∏—Ç—å max_tokens –¥–æ 500
- [ ] (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –î–æ–±–∞–≤–∏—Ç—å streaming

### Phase 4: –ú–µ—Ç—Ä–∏–∫–∏ (0.5 –¥–Ω—è)

- [ ] Prometheus metrics
- [ ] OpenTelemetry tracing
- [ ] Dashboard –≤ Grafana

---

## üìä –ú–ï–¢–†–ò–ö–ò –î–õ–Ø –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–ò–Ø

### Prometheus Metrics:

```python
from prometheus_client import Histogram, Counter

# Latency histograms
retrieval_latency = Histogram(
    'retrieval_latency_seconds',
    'Retrieval latency in seconds',
    buckets=[0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
)

gpt_latency = Histogram(
    'gpt_latency_seconds',
    'GPT latency in seconds',
    buckets=[0.5, 1.0, 2.0, 3.0, 5.0, 10.0]
)

db_latency = Histogram(
    'db_save_latency_seconds',
    'DB save latency in seconds',
    buckets=[0.001, 0.01, 0.1, 0.5, 1.0]
)

total_latency = Histogram(
    'request_latency_seconds',
    'Total request latency',
    buckets=[0.5, 1.0, 2.0, 3.0, 5.0, 10.0, 30.0]
)

# Counters
cache_hits = Counter('cache_hits_total', 'Cache hits')
cache_misses = Counter('cache_misses_total', 'Cache misses')
```

### Usage:

```python
import time

async def handle_question(message: Message):
    start = time.time()
    
    # Retrieval
    ret_start = time.time()
    results = await fast_retrieval(query, language)
    retrieval_latency.observe(time.time() - ret_start)
    
    # GPT
    gpt_start = time.time()
    response = await llm_router.process_query(...)
    gpt_latency.observe(time.time() - gpt_start)
    
    # DB (async)
    db_start = time.time()
    await message_queue.enqueue(...)
    db_latency.observe(time.time() - db_start)
    
    # Total
    total_latency.observe(time.time() - start)
```

---

## üéØ ACCEPTANCE CRITERIA

```
‚úÖ p50 < 2.0s
‚úÖ p95 < 5.0s
‚úÖ Retrieval < 600ms (p95)
‚úÖ GPT < 3.0s (p95)
‚úÖ DB < 150ms (p95)
‚úÖ Cache hit rate > 30%
‚úÖ No HIGH security issues
‚úÖ MI ‚â• 70 (B) –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
‚úÖ Test coverage ‚â• 50%
```

**–¢–æ–ª—å–∫–æ –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ - "production-ready –∑–∞ $8-12k"!**

---

**DEADLINE:** 3 –¥–Ω—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã

**–ì–æ—Ç–æ–≤ –Ω–∞—á–∏–Ω–∞—Ç—å?** üöÄ

